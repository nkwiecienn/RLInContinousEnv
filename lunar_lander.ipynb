{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Due to many problems it is strongly advised to use Python 3.9 and the packages' versions specified in the requirements.**",
   "id": "5ca975f56e6fe989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:21:25.636406Z",
     "start_time": "2025-05-31T08:21:25.592057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import cv2\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n",
    "from tbparse import SummaryReader"
   ],
   "id": "5a9ec3ec31dbf47a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example for a game with random moves",
   "id": "490bf70da7f0313b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T17:53:42.466605Z",
     "start_time": "2025-05-30T17:53:36.062548Z"
    }
   },
   "source": [
    "env = gym.make(\n",
    "    \"LunarLanderContinuous-v3\",\n",
    "    continuous = True,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "state, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    frame = env.render()\n",
    "    cv2.imshow(\"frame\", cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(50)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example for using SAC from stable_baselines3 for the Lunar Lander Problem",
   "id": "bb06297286e746cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T17:56:13.957359Z",
     "start_time": "2025-05-30T17:53:42.529584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)"
   ],
   "id": "7d750e36bd8bf641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -246     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 443      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.184    |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.908    |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 342      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | -163     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 87       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1021     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.24     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.776    |\n",
      "|    ent_coef_loss   | -0.591   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 920      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | -161     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 1810     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0887   |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.626    |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 170      |\n",
      "|    ep_rew_mean     | -177     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 2716     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.419    |\n",
      "|    critic_loss     | 13.8     |\n",
      "|    ent_coef        | 0.491    |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2615     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 263      |\n",
      "|    ep_rew_mean     | -169     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 5268     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.58    |\n",
      "|    critic_loss     | 3.05     |\n",
      "|    ent_coef        | 0.265    |\n",
      "|    ent_coef_loss   | -1.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5167     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 383      |\n",
      "|    ep_rew_mean     | -140     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 67       |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 9191     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.17    |\n",
      "|    critic_loss     | 3.87     |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9090     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x24f9b55f970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T17:56:21.500520Z",
     "start_time": "2025-05-30T17:56:14.039142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "obs, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    frame = env.render()\n",
    "    cv2.imshow(\"frame\", cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(50)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "793e24a8c7b452f4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "The best hyperparameters for this problem can be found at https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml:\n",
    "\n",
    "```yml\n",
    "LunarLanderContinuous-v3:\n",
    "  n_timesteps: !!float 5e5\n",
    "  policy: 'MlpPolicy'\n",
    "  batch_size: 256\n",
    "  learning_rate: lin_7.3e-4\n",
    "  buffer_size: 1000000\n",
    "  ent_coef: 'auto'\n",
    "  gamma: 0.99\n",
    "  tau: 0.01\n",
    "  train_freq: 1\n",
    "  gradient_steps: 1\n",
    "  learning_starts: 10000\n",
    "  policy_kwargs: \"dict(net_arch=[400, 300])\"\n",
    "```\n",
    "\n",
    "It is safe to assume that those are the most important hyperparameters for training. Here is a brief description of each one:\n",
    "- *n_timesteps* - total number of time steps to train the model;\n",
    "- *policy* - choice of neural network (MlpPolicy means Multi-Layer Perceptron policy);\n",
    "- *policy_kwargs* - the architecture of the network (here [400, 300] means two hidden layers with 400 and 300 neurons);\n",
    "- *batch_size* - number of samples per training update;\n",
    "- *learning_rate* - a linear schedule for learning rate (here starting at 7.3e-3 and decreasing linearly to 0 over training);\n",
    "- *learning_starts* - the agent collects n steps before starting to learn;\n",
    "- *ent-coef* - balance between exploration and exploitation (here is learnt automatically);\n",
    "- *gamma* - discount factor for future rewards;\n",
    "- *tau* - controls the soft update speed of the target networks;\n",
    "- *train_freq* - the model is trained every n steps;\n",
    "- *gradient_steps* - how many gradient steps are done after each rollout."
   ],
   "id": "1f799168e760edb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Learning curves",
   "id": "7dbdd7d8eecf740f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T02:12:07.743602Z",
     "start_time": "2025-05-30T18:57:13.014341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_runs = 10\n",
    "timesteps = 50_000\n",
    "\n",
    "param_sets = [\n",
    "    {\"learning_rate\": 3e-4, \"batch_size\": 256},\n",
    "    {\"learning_rate\": 7.3e-4, \"batch_size\": 256},\n",
    "    {\"learning_rate\": 1e-3, \"batch_size\": 128},\n",
    "]\n",
    "\n",
    "log_root = \"./logs\"\n",
    "\n",
    "for i, params in enumerate(param_sets):\n",
    "    for run in range(n_runs):\n",
    "        env = gym.make(\"LunarLanderContinuous-v3\", render_mode=\"rgb_array\")\n",
    "        env = Monitor(env)\n",
    "\n",
    "        model = SAC(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=0,\n",
    "            seed=run,\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            batch_size=params[\"batch_size\"],\n",
    "            tensorboard_log=f\"{log_root}/set_{i}\",\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=timesteps, tb_log_name=f\"run_{run}\")\n",
    "        env.close()"
   ],
   "id": "75ccfff6de4b849c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T08:36:14.555433Z",
     "start_time": "2025-05-31T08:36:13.399035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_root = \"./logs\"\n",
    "param_sets = 3\n",
    "output_dir = \"./plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def load_rewards_from_tensorboard(log_dir):\n",
    "    rewards = []\n",
    "    for run_dir in os.listdir(log_dir):\n",
    "        full_path = os.path.join(log_dir, run_dir)\n",
    "        reader = SummaryReader(full_path)\n",
    "        df = reader.scalars\n",
    "        reward_df = df[df['tag'] == 'rollout/ep_rew_mean']\n",
    "        rewards.append((reward_df['step'].values, reward_df['value'].values))\n",
    "\n",
    "    return rewards\n",
    "\n",
    "all_sets_data = []\n",
    "\n",
    "for i in range(param_sets):\n",
    "    reward_curves = load_rewards_from_tensorboard(f\"{log_root}/set_{i}\")\n",
    "\n",
    "    all_x = sorted(set(x for steps, _ in reward_curves for x in steps))\n",
    "    all_x = np.array(all_x)\n",
    "\n",
    "    aligned_rewards = []\n",
    "    for steps, values in reward_curves:\n",
    "        interp_values = np.interp(all_x, steps, values)\n",
    "        aligned_rewards.append(interp_values)\n",
    "\n",
    "    aligned_rewards = np.array(aligned_rewards)\n",
    "    mean_rewards = np.mean(aligned_rewards, axis=0)\n",
    "    std_rewards = np.std(aligned_rewards, axis=0)\n",
    "\n",
    "    all_sets_data.append((all_x, mean_rewards, std_rewards))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_x, mean_rewards, label=f\"Set {i}\")\n",
    "    plt.fill_between(all_x, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.3)\n",
    "    plt.title(f\"Learning Curve for Hyperparameter Set {i}\")\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Episode Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/set_{i}.png\")\n",
    "    plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (x, mean, std) in enumerate(all_sets_data):\n",
    "    plt.plot(x, mean, label=f\"Set {i}\")\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.3)\n",
    "plt.title(\"Combined Learning Curves\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Episode Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/combined.png\")\n",
    "plt.close()"
   ],
   "id": "5020973ca6b88bc6",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
